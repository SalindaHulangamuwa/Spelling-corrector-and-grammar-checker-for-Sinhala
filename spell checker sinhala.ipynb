{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a35b30d8-fe70-4305-be39-f8b9bf43cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 67260 correct Sinhala words.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel dataset\n",
    "file_path = \"data-spell-checker.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Extract correct words\n",
    "dictionary = data[data['label'] == 1]['word'].tolist()\n",
    "print(f\"Loaded {len(dictionary)} correct Sinhala words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6e0fac-42d2-493d-a590-21ef0155bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sinhala stop words\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4affbe2b-693f-4bff-99ce-18bd23af4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    # Use regex to remove non-alphanumeric characters except spaces\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation (keeping spaces)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec106113-e843-4ff6-a223-39284d6dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinhala_stemmer(word):\n",
    "    suffixes = ['ින්', 'ට', 'ව', 'ගේ', 'යන්', 'නවා']  # Add more relevant suffixes\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fbd918d-7188-49f8-9cbc-3c0283a4af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spelling_errors(words, dictionary):\n",
    "    # Find words not in the dictionary\n",
    "    errors = [word for word in words if word not in dictionary]\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd125c02-89c5-4c4a-a712-38f17e8927e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def sinhala_soundex(word):\n",
    "    phonetic_map = {\n",
    "        'ක': '1', 'ඛ': '1',\n",
    "        'ච': '2', 'ජ': '2', 'ඡ': '2', 'ඣ': '2',\n",
    "        'ට': '3', 'ඩ': '3', 'ඨ': '3', 'ඪ': '3',\n",
    "        'ත': '4', 'ථ': '4',\n",
    "        'බ': '5', 'ඵ': '5', 'භ': '5',\n",
    "        'ශ': '7', 'ෂ': '7', 'ස': '7',\n",
    "        'ග': '8', 'ඝ': '8', 'ඟ': '8'\n",
    "    }\n",
    "    first_letter = word[0]\n",
    "    soundex_code = [first_letter]\n",
    "    for char in word[1:]:\n",
    "        if char in phonetic_map:\n",
    "            code = phonetic_map[char]\n",
    "            if soundex_code[-1] != code:\n",
    "                soundex_code.append(code)\n",
    "    while len(soundex_code) < 4:\n",
    "        soundex_code.append('0')  # Pad with zeros\n",
    "    return ''.join(soundex_code[:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7692da54-010d-491a-a0d1-da1ea6938c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections(errors, dictionary, limit=3, threshold=80):\n",
    "    suggestions = {}\n",
    "    for error in errors:\n",
    "        matches = process.extract(error, dictionary, limit=limit)\n",
    "        # Filter suggestions based on minimum similarity threshold\n",
    "        filtered_matches = [match[0] for match in matches if match[1] >= threshold]\n",
    "        suggestions[error] = filtered_matches\n",
    "    return suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38c33d4c-a1f5-45ab-b35a-adf6fd325a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(input_text, dictionary, stopwords):\n",
    "    # Step 1: Preprocess input text\n",
    "    words = preprocess_text(input_text, stopwords)\n",
    "    # Step 2: Detect spelling errors\n",
    "    errors = detect_spelling_errors(words, dictionary)\n",
    "    \n",
    "    if not errors:\n",
    "        return \"No spelling errors found!\", {}\n",
    "\n",
    "    # Step 3: Suggest corrections for detected errors\n",
    "    corrections = suggest_corrections(errors, dictionary)\n",
    "    return errors, corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328936b9-98af-461d-83a7-9ceb36d1cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(input_text, dictionary, stopwords):\n",
    "    errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
    "    words = input_text.split()\n",
    "\n",
    "    # Replace each word with the top suggestion if available\n",
    "    corrected_words = [\n",
    "        corrections.get(word, [word])[0]  # If word is found in corrections, replace it\n",
    "        for word in words\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "408667bd-6f31-44e2-b7fb-ce156f12c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: ['කක']\n",
      "Suggestions: {'කක': ['අධිකකම', 'අවංකකම', 'එකක්වත්']}\n",
      "Corrected Text: කේක\n"
     ]
    }
   ],
   "source": [
    "input_text = \"කේක\"\n",
    "\n",
    "# Run the spell checker\n",
    "errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
    "print(\"Errors:\", errors)\n",
    "print(\"Suggestions:\", corrections)\n",
    "\n",
    "# Auto-correct the text\n",
    "corrected_text = auto_correct(input_text, dictionary, stopwords)\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9046ac9e-0808-4291-bd24-77dc19a8d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Errors: {'අදරය': [('දරය', 86.0), ('අරය', 86.0), ('අධරය', 85.0), ('අන්දරය', 80.0), ('අනාදරය', 80.0)]}\n",
      "Corrected Text: දරය\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class AdvancedSinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str):\n",
    "        # Load dictionary with more sophisticated preprocessing\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Suffix rules for more comprehensive stemming\n",
    "        self.suffix_rules = [\n",
    "            'ආගම', 'ගෙන', 'යෙහි', 'යේ', 'ට', 'ම', \n",
    "            'යන', 'ක', 'වා', 'ලා', 'ල', 'න', 'හි'\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "      \n",
    "        # Assuming 'word' column contains correct words and 'label' column indicates correctness\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "       \n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "      \n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "     \n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "    \n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Stemming\n",
    "        stemmed_word = self._advanced_stemmer(word)\n",
    "        \n",
    "        # Multiple similarity strategies\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            # Phonetic similarity\n",
    "            phonetic_similarity = fuzz.ratio(\n",
    "                self._phonetic_key(stemmed_word), \n",
    "                self._phonetic_key(dict_word)\n",
    "            )\n",
    "            \n",
    "            # String-based similarity\n",
    "            string_similarity = fuzz.ratio(word, dict_word)\n",
    "            \n",
    "            # Levenshtein distance\n",
    "            edit_similarity = fuzz.token_sort_ratio(word, dict_word)\n",
    "            \n",
    "            # Combined weighted similarity\n",
    "            combined_score = (\n",
    "                0.4 * phonetic_similarity + \n",
    "                0.3 * string_similarity + \n",
    "                0.3 * edit_similarity\n",
    "            )\n",
    "            \n",
    "            candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort and filter candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [\n",
    "            (candidate, score) \n",
    "            for candidate, score in candidates \n",
    "            if score >= threshold\n",
    "        ][:limit]\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "       \n",
    "        # Preprocess text\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        # Spelling error detection and correction\n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "      \n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                # Take the first (best) suggestion\n",
    "                corrected_words.append(errors[word][0][0])\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker\n",
    "    spell_checker = AdvancedSinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt'\n",
    "    )\n",
    "    \n",
    "    # Test input\n",
    "    test_text = \"අදරය\"\n",
    "    \n",
    "    # Spell check\n",
    "    spelling_errors = spell_checker.spell_check(test_text)\n",
    "    print(\"Spelling Errors:\", spelling_errors)\n",
    "    \n",
    "    # Auto-correction\n",
    "    corrected_text = spell_checker.auto_correct(test_text)\n",
    "    print(\"Corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553d2dd-89a5-4276-944d-fe47b2793680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: Could not find a suitable TLS CA certificate bundle, invalid path: C:\\Program Files\\PostgreSQL\\16\\ssl\\certs\\ca-bundle.crt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install sinling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8906f93e-b8b4-4663-a1de-7635d1404507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('මම', 'PRP'), ('පොත', 'NNC'), ('කියවායි', 'VFM'), ('.', 'FS')]]\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "\n",
    "tokenizer = SinhalaTokenizer()\n",
    "tagger = POSTagger()\n",
    "\n",
    "document = \"මම පොත කියවායි\"\n",
    "tokenized_sentences = [tokenizer.tokenize(f'{ss}.') for ss in tokenizer.split_sentences(document)]\n",
    "pos_tags = tagger.predict(tokenized_sentences)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "763f9a3a-0cbc-4ed0-ba0a-ee3c668ff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියවනවා', 84.77222222222221), ('කියවීම', 78.5), ('කියවෙනවා', 78.5), ('කියවනය', 76.85), ('කියමු', 72.9)]}\n",
      "Auto-corrected Text: මම පොත කියවනවා\n",
      "\n",
      "--- Spell Check for: ගෘහෂ්ථ\n",
      "Spelling Errors: {'ගෘහෂ්ථ': [('ගෘහස්ථ', 82.38333333333333)]}\n",
      "Auto-corrected Text: ගෘහස්ථ\n",
      "\n",
      "--- Spell Check for: අදරය\n",
      "Spelling Errors: {'අදරය': [('ආදරය', 92.5), ('අර', 74.0), ('ආර', 74.0), ('දර', 74.0), ('අද', 74.0)]}\n",
      "Auto-corrected Text: ආදරය\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes from suffixes_list.txt\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            self.suffix_rules = file.read().splitlines()\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies\n",
    "        \"\"\"\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker with suffix list file path\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt'\n",
    "    )\n",
    "    \n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\",\n",
    "        \"ගෘහෂ්ථ\",\n",
    "        \"අදරය\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dadc88f-3f68-4f92-9df6-8d2de6050975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියවනවා', 84.77222222222221), ('කියවීම', 78.5), ('කියවනය', 76.85), ('කියමු', 72.9), ('කියනවා', 72.9)]}\n",
      "Auto-corrected Text: මම පොත කියවනවා\n"
     ]
    }
   ],
   "source": [
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str, stem_dictionary_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes from suffixes_list.txt\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            self.suffix_rules = file.read().splitlines()\n",
    "        \n",
    "        # Load stem dictionary\n",
    "        self.stem_dictionary = self._load_stem_dictionary(stem_dictionary_path)\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _load_stem_dictionary(self, stem_dictionary_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Load stem dictionary from a file, where each line contains a word variation and its stem.\n",
    "        \"\"\"\n",
    "        stem_dict = {}\n",
    "        with open(stem_dictionary_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                word, stem = line.strip().split('\\t')\n",
    "                stem_dict[word] = stem\n",
    "        return stem_dict\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies and stem dictionary\n",
    "        \"\"\"\n",
    "        # First, check if the word exists in the stem dictionary\n",
    "        if word in self.stem_dictionary:\n",
    "            return self.stem_dictionary[word]\n",
    "        \n",
    "        # If not, apply suffix removal rules\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker with suffix list and stem dictionary file paths\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt',\n",
    "        stem_dictionary_path='stem_dictionary.txt'\n",
    "    )\n",
    "    \n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d52a1d32-4171-4b96-8c6c-a724eff5d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar error: Verb '.' should end with 'මි' when the subject is 'මම'.\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "\n",
    "class SinhalaGrammarChecker:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = SinhalaTokenizer()\n",
    "        self.tagger = POSTagger()\n",
    "\n",
    "    def is_sov_order(self, pos_tags):\n",
    "        \"\"\"\n",
    "        Check if the sentence follows SOV order based on POS tags.\n",
    "        \"\"\"\n",
    "        if len(pos_tags) < 3:\n",
    "            return False  # Sentence too short to be SOV\n",
    "        \n",
    "        subject_tag, object_tag, verb_tag = pos_tags[0][1], pos_tags[1][1], pos_tags[2][1]\n",
    "        \n",
    "        # SOV structure: S -> PRP, O -> NNC, V -> V* (verbs starting with 'V')\n",
    "        return subject_tag == 'PRP' and object_tag == 'NNC' and verb_tag.startswith('V')\n",
    "\n",
    "    def check_grammar(self, sentence):\n",
    "        \"\"\"\n",
    "        Check grammar rules for a given sentence.\n",
    "        \"\"\"\n",
    "        tokenized_sentences = [self.tokenizer.tokenize(f'{ss}.') for ss in self.tokenizer.split_sentences(sentence)]\n",
    "        pos_tags = self.tagger.predict(tokenized_sentences)\n",
    "        \n",
    "        if not pos_tags or not pos_tags[0]:\n",
    "            return \"Unable to analyze the sentence.\"\n",
    "        \n",
    "        tokens = tokenized_sentences[0]\n",
    "        tags = pos_tags[0]\n",
    "        \n",
    "        # Ensure the sentence follows SOV structure\n",
    "        if not self.is_sov_order(tags):\n",
    "            return \"Sentence does not follow SOV order.\"\n",
    "        \n",
    "        # Extract Subject, Verb, and Object\n",
    "        subject = tokens[0]\n",
    "        verb = tokens[-1]\n",
    "        \n",
    "        # Rule 1: If S = \"මම\", V should end with \"මි\"\n",
    "        if subject == \"මම\" and not verb.endswith(\"මි\"):\n",
    "            return f\"Grammar error: Verb '{verb}' should end with 'මි' when the subject is 'මම'.\"\n",
    "        \n",
    "        # Rule 2: If S = \"අපි\", V should end with \"මු\"\n",
    "        if subject == \"අපි\" and not verb.endswith(\"මු\"):\n",
    "            return f\"Grammar error: Verb '{verb}' should end with 'මු' when the subject is 'අපි'.\"\n",
    "        \n",
    "        return \"The sentence is grammatically correct.\"\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    grammar_checker = SinhalaGrammarChecker()\n",
    "\n",
    "    sentence = \"මම පොත කියවායි\"\n",
    "    result = grammar_checker.check_grammar(sentence)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b610e661-8cce-479b-a931-fdeafa7dbe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: අපි පොත කියවම\n",
      "✗ Grammar errors found:\n",
      "  - Sentence does not follow Subject-Object-Verb order\n",
      "  - Verb ending doesn't agree with subject 'අපි'\n",
      "\n",
      "Suggestions:\n",
      "  - Consider: අපි පොත කියමු\n",
      "\n",
      "Analyzing: මම පාඩම කරයි\n",
      "✗ Grammar errors found:\n",
      "  - Verb ending doesn't agree with subject 'මම'\n",
      "\n",
      "Suggestions:\n",
      "  - Consider: මම පාඩම කරමි\n",
      "\n",
      "Analyzing: ඔහු සිංහල ඉගෙනගනී\n",
      "✗ Grammar errors found:\n",
      "  - Sentence does not follow Subject-Object-Verb order\n",
      "  - Verb ending doesn't agree with subject 'ඔහු'\n",
      "\n",
      "Suggestions:\n",
      "  - Consider: ඔහු සිංහල ඉගෙනගයි\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class SinhalaGrammarChecker:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the grammar checker with required tools.\"\"\"\n",
    "        self.tokenizer = SinhalaTokenizer()\n",
    "        self.tagger = POSTagger()\n",
    "        \n",
    "        # Define verb endings for different subjects\n",
    "        self.subject_verb_endings = {\n",
    "            \"මම\": \"මි\",    # I\n",
    "            \"අපි\": \"මු\",    # We\n",
    "            \"ඔහු\": \"යි\",    # He\n",
    "            \"ඇය\": \"යි\",    # She\n",
    "            \"ඔවුන්\": \"ති\",  # They\n",
    "            \"ඔබ\": \"හි\",     # You (singular)\n",
    "            \"ඔබලා\": \"හු\",   # You (plural)\n",
    "        }\n",
    "        \n",
    "    def tokenize_and_tag(self, sentence: str) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
    "        \"\"\"\n",
    "        Tokenize and POS tag the input sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Input Sinhala sentence\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (tokens, pos_tags)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Add period if sentence doesn't end with punctuation\n",
    "            if not sentence[-1] in [\".\", \"!\", \"?\"]:\n",
    "                sentence += \".\"\n",
    "                \n",
    "            tokenized_sentences = [self.tokenizer.tokenize(ss) \n",
    "            for ss in self.tokenizer.split_sentences(sentence)]\n",
    "            if not tokenized_sentences:\n",
    "                raise ValueError(\"Empty sentence after tokenization\")\n",
    "                \n",
    "            pos_tags = self.tagger.predict(tokenized_sentences)\n",
    "            if not pos_tags or not pos_tags[0]:\n",
    "                raise ValueError(\"Failed to generate POS tags\")\n",
    "                \n",
    "            return tokenized_sentences[0], pos_tags[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in tokenization/tagging: {str(e)}\")\n",
    "\n",
    "    def is_sov_order(self, tokens: List[str], pos_tags: List[Tuple[str, str]]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the sentence follows Subject-Object-Verb order.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokenized words\n",
    "            pos_tags: List of POS tags for each token\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating if sentence follows SOV order\n",
    "        \"\"\"\n",
    "        if len(pos_tags) < 3:\n",
    "            return False\n",
    "\n",
    "        # Get basic components\n",
    "        subject_pos = pos_tags[0][1]\n",
    "        verb_pos = pos_tags[-1][1]\n",
    "        \n",
    "        # Check if there's an object between subject and verb\n",
    "        has_object = False\n",
    "        for tag in pos_tags[1:-1]:\n",
    "            if tag[1] in ['NNC', 'NNP', 'PRP']:  # Common noun, proper noun, or pronoun\n",
    "                has_object = True\n",
    "                break\n",
    "                \n",
    "        return (subject_pos in ['PRP', 'NNP', 'NNC'] and  # Subject is pronoun or noun\n",
    "                verb_pos.startswith('V') and  # Last word is verb\n",
    "                has_object)  # Has object between S and V\n",
    "\n",
    "    def check_subject_verb_agreement(self, subject: str, verb: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Check if the verb ending agrees with the subject.\n",
    "        \n",
    "        Args:\n",
    "            subject: Subject word\n",
    "            verb: Verb word\n",
    "            \n",
    "        Returns:\n",
    "            Correction suggestion if there's an error, None if correct\n",
    "        \"\"\"\n",
    "        if subject in self.subject_verb_endings:\n",
    "            expected_ending = self.subject_verb_endings[subject]\n",
    "            if not verb.endswith(expected_ending):\n",
    "                # Generate correct verb form\n",
    "                verb_root = verb[:-2] if len(verb) > 2 else verb\n",
    "                correct_verb = verb_root + expected_ending\n",
    "                return correct_verb\n",
    "        return None\n",
    "\n",
    "    def check_grammar(self, sentence: str) -> dict:\n",
    "        \"\"\"\n",
    "        Check grammar rules for a given sentence and return detailed analysis.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Input Sinhala sentence\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing analysis results and suggestions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens, pos_tags = self.tokenize_and_tag(sentence)\n",
    "            \n",
    "            result = {\n",
    "                \"original\": sentence,\n",
    "                \"is_grammatical\": True,\n",
    "                \"errors\": [],\n",
    "                \"suggestions\": [],\n",
    "                \"analysis\": {\n",
    "                    \"tokens\": tokens,\n",
    "                    \"pos_tags\": pos_tags\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Check word order\n",
    "            if not self.is_sov_order(tokens, pos_tags):\n",
    "                result[\"is_grammatical\"] = False\n",
    "                result[\"errors\"].append(\"Sentence does not follow Subject-Object-Verb order\")\n",
    "                # Suggest correction by reordering\n",
    "                suggested_order = self._reorder_to_sov(tokens, pos_tags)\n",
    "                if suggested_order:\n",
    "                    result[\"suggestions\"].append(f\"Consider: {' '.join(suggested_order)}\")\n",
    "            \n",
    "            # Check subject-verb agreement\n",
    "            subject = tokens[0]\n",
    "            verb = tokens[-1]\n",
    "            corrected_verb = self.check_subject_verb_agreement(subject, verb)\n",
    "            \n",
    "            if corrected_verb:\n",
    "                result[\"is_grammatical\"] = False\n",
    "                result[\"errors\"].append(f\"Verb ending doesn't agree with subject '{subject}'\")\n",
    "                # Create corrected sentence\n",
    "                corrected_tokens = tokens[:-1] + [corrected_verb]\n",
    "                result[\"suggestions\"].append(f\"Consider: {' '.join(corrected_tokens)}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except ValueError as e:\n",
    "            return {\n",
    "                \"original\": sentence,\n",
    "                \"is_grammatical\": False,\n",
    "                \"errors\": [str(e)],\n",
    "                \"suggestions\": [],\n",
    "                \"analysis\": None\n",
    "            }\n",
    "\n",
    "    def _reorder_to_sov(self, tokens: List[str], pos_tags: List[Tuple[str, str]]) -> Optional[List[str]]:\n",
    "        \"\"\"\n",
    "        Attempt to reorder tokens to follow SOV order.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokens\n",
    "            pos_tags: List of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Reordered list of tokens if possible, None if not\n",
    "        \"\"\"\n",
    "        # Find subject, object, and verb candidates\n",
    "        subject_idx = None\n",
    "        object_idx = None\n",
    "        verb_idx = None\n",
    "        \n",
    "        for i, (token, tag) in enumerate(zip(tokens, pos_tags)):\n",
    "            if tag[1] in ['PRP', 'NNP', 'NNC'] and subject_idx is None:\n",
    "                subject_idx = i\n",
    "            elif tag[1] in ['NNC', 'NNP', 'PRP'] and subject_idx is not None and object_idx is None:\n",
    "                object_idx = i\n",
    "            elif tag[1].startswith('V'):\n",
    "                verb_idx = i\n",
    "        \n",
    "        if all(x is not None for x in [subject_idx, object_idx, verb_idx]):\n",
    "            # Reorder maintaining other words' relative positions\n",
    "            reordered = []\n",
    "            # Add subject\n",
    "            reordered.append(tokens[subject_idx])\n",
    "            # Add object\n",
    "            reordered.append(tokens[object_idx])\n",
    "            # Add any intervening words\n",
    "            for i, token in enumerate(tokens):\n",
    "                if i not in [subject_idx, object_idx, verb_idx]:\n",
    "                    reordered.append(token)\n",
    "            # Add verb at end\n",
    "            reordered.append(tokens[verb_idx])\n",
    "            return reordered\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the grammar checker.\"\"\"\n",
    "    checker = SinhalaGrammarChecker()\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"අපි පොත කියවම\",\n",
    "        \"මම පාඩම කරයි\",\n",
    "        \"ඔහු සිංහල ඉගෙනගනී\",\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        print(\"\\nAnalyzing:\", sentence)\n",
    "        result = checker.check_grammar(sentence)\n",
    "        \n",
    "        if result[\"is_grammatical\"]:\n",
    "            print(\"✓ Grammatically correct\")\n",
    "        else:\n",
    "            print(\"✗ Grammar errors found:\")\n",
    "            for error in result[\"errors\"]:\n",
    "                print(f\"  - {error}\")\n",
    "            print(\"\\nSuggestions:\")\n",
    "            for suggestion in result[\"suggestions\"]:\n",
    "                print(f\"  - {suggestion}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14425cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
