{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35b30d8-fe70-4305-be39-f8b9bf43cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 67260 correct Sinhala words.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel dataset\n",
    "file_path = \"data-spell-checker.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Extract correct words\n",
    "dictionary = data[data['label'] == 1]['word'].tolist()\n",
    "print(f\"Loaded {len(dictionary)} correct Sinhala words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6e0fac-42d2-493d-a590-21ef0155bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sinhala stop words\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4affbe2b-693f-4bff-99ce-18bd23af4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    # Use regex to remove non-alphanumeric characters except spaces\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation (keeping spaces)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec106113-e843-4ff6-a223-39284d6dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinhala_stemmer(word):\n",
    "    suffixes = ['ින්', 'ට', 'ව', 'ගේ', 'යන්', 'නවා']  # Add more relevant suffixes\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fbd918d-7188-49f8-9cbc-3c0283a4af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spelling_errors(words, dictionary):\n",
    "    # Find words not in the dictionary\n",
    "    errors = [word for word in words if word not in dictionary]\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd125c02-89c5-4c4a-a712-38f17e8927e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def sinhala_soundex(word):\n",
    "    phonetic_map = {\n",
    "        'ක': '1', 'ඛ': '1',\n",
    "        'ච': '2', 'ජ': '2', 'ඡ': '2', 'ඣ': '2',\n",
    "        'ට': '3', 'ඩ': '3', 'ඨ': '3', 'ඪ': '3',\n",
    "        'ත': '4', 'ථ': '4',\n",
    "        'බ': '5', 'ඵ': '5', 'භ': '5',\n",
    "        'ශ': '7', 'ෂ': '7', 'ස': '7',\n",
    "        'ග': '8', 'ඝ': '8', 'ඟ': '8'\n",
    "    }\n",
    "    first_letter = word[0]\n",
    "    soundex_code = [first_letter]\n",
    "    for char in word[1:]:\n",
    "        if char in phonetic_map:\n",
    "            code = phonetic_map[char]\n",
    "            if soundex_code[-1] != code:\n",
    "                soundex_code.append(code)\n",
    "    while len(soundex_code) < 4:\n",
    "        soundex_code.append('0')  # Pad with zeros\n",
    "    return ''.join(soundex_code[:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7692da54-010d-491a-a0d1-da1ea6938c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections(errors, dictionary, limit=3, threshold=80):\n",
    "    suggestions = {}\n",
    "    for error in errors:\n",
    "        matches = process.extract(error, dictionary, limit=limit)\n",
    "        # Filter suggestions based on minimum similarity threshold\n",
    "        filtered_matches = [match[0] for match in matches if match[1] >= threshold]\n",
    "        suggestions[error] = filtered_matches\n",
    "    return suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38c33d4c-a1f5-45ab-b35a-adf6fd325a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(input_text, dictionary, stopwords):\n",
    "    # Step 1: Preprocess input text\n",
    "    words = preprocess_text(input_text, stopwords)\n",
    "    # Step 2: Detect spelling errors\n",
    "    errors = detect_spelling_errors(words, dictionary)\n",
    "    \n",
    "    if not errors:\n",
    "        return \"No spelling errors found!\", {}\n",
    "\n",
    "    # Step 3: Suggest corrections for detected errors\n",
    "    corrections = suggest_corrections(errors, dictionary)\n",
    "    return errors, corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "328936b9-98af-461d-83a7-9ceb36d1cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(input_text, dictionary, stopwords):\n",
    "    errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
    "    words = input_text.split()\n",
    "\n",
    "    # Replace each word with the top suggestion if available\n",
    "    corrected_words = [\n",
    "        corrections.get(word, [word])[0]  # If word is found in corrections, replace it\n",
    "        for word in words\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "408667bd-6f31-44e2-b7fb-ce156f12c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: ['කක']\n",
      "Suggestions: {'කක': ['අධිකකම', 'අවංකකම', 'එකක්වත්']}\n",
      "Corrected Text: කේක\n"
     ]
    }
   ],
   "source": [
    "input_text = \"කේක\"\n",
    "\n",
    "# Run the spell checker\n",
    "errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
    "print(\"Errors:\", errors)\n",
    "print(\"Suggestions:\", corrections)\n",
    "\n",
    "# Auto-correct the text\n",
    "corrected_text = auto_correct(input_text, dictionary, stopwords)\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9046ac9e-0808-4291-bd24-77dc19a8d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Errors: {'අදරය': [('අරය', 86.0), ('දරය', 86.0), ('අධරය', 85.0), ('අන්දරය', 80.0), ('අනාදරය', 80.0)]}\n",
      "Corrected Text: අරය\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class AdvancedSinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str):\n",
    "        # Load dictionary with more sophisticated preprocessing\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Suffix rules for more comprehensive stemming\n",
    "        self.suffix_rules = [\n",
    "            'ආගම', 'ගෙන', 'යෙහි', 'යේ', 'ට', 'ම', \n",
    "            'යන', 'ක', 'වා', 'ලා', 'ල', 'න', 'හි'\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "      \n",
    "        # Assuming 'word' column contains correct words and 'label' column indicates correctness\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "       \n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "      \n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "     \n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "    \n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Stemming\n",
    "        stemmed_word = self._advanced_stemmer(word)\n",
    "        \n",
    "        # Multiple similarity strategies\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            # Phonetic similarity\n",
    "            phonetic_similarity = fuzz.ratio(\n",
    "                self._phonetic_key(stemmed_word), \n",
    "                self._phonetic_key(dict_word)\n",
    "            )\n",
    "            \n",
    "            # String-based similarity\n",
    "            string_similarity = fuzz.ratio(word, dict_word)\n",
    "            \n",
    "            # Levenshtein distance\n",
    "            edit_similarity = fuzz.token_sort_ratio(word, dict_word)\n",
    "            \n",
    "            # Combined weighted similarity\n",
    "            combined_score = (\n",
    "                0.4 * phonetic_similarity + \n",
    "                0.3 * string_similarity + \n",
    "                0.3 * edit_similarity\n",
    "            )\n",
    "            \n",
    "            candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort and filter candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [\n",
    "            (candidate, score) \n",
    "            for candidate, score in candidates \n",
    "            if score >= threshold\n",
    "        ][:limit]\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "       \n",
    "        # Preprocess text\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        # Spelling error detection and correction\n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "      \n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                # Take the first (best) suggestion\n",
    "                corrected_words.append(errors[word][0][0])\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker\n",
    "    spell_checker = AdvancedSinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt'\n",
    "    )\n",
    "    \n",
    "    # Test input\n",
    "    test_text = \"අදරය\"\n",
    "    \n",
    "    # Spell check\n",
    "    spelling_errors = spell_checker.spell_check(test_text)\n",
    "    print(\"Spelling Errors:\", spelling_errors)\n",
    "    \n",
    "    # Auto-correction\n",
    "    corrected_text = spell_checker.auto_correct(test_text)\n",
    "    print(\"Corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "564b94ae-3bc0-4fe9-9182-5ebdbbcec556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියවනය', 71.43076923076923), ('කියැවිය', 71.10714285714286)]}\n",
      "Auto-corrected Text: මම පොත කියවනය\n",
      "\n",
      "--- Spell Check for: ගෘහෂ්ථ\n",
      "Spelling Errors: {'ගෘහෂ්ථ': [('ගෘහස්ථ', 82.38333333333333)]}\n",
      "Auto-corrected Text: ගෘහස්ථ\n",
      "\n",
      "--- Spell Check for: වෙරදි\n",
      "Spelling Errors: {'වෙරදි': [('වැරදි', 76.025), ('වෙරදරණ', 70.53181818181818)]}\n",
      "Auto-corrected Text: වැරදි\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import difflib\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "        \n",
    "        # Comprehensive suffix rules\n",
    "        self.suffix_rules = [\n",
    "             'ගෙන', 'යෙහි', 'යේ', 'ට', 'ම', \n",
    "            'යන', 'ක', 'වා', 'ලා', 'ල', 'න', 'හි'\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        # Assuming 'word' column contains words and 'label' column indicates correctness\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies\n",
    "        \"\"\"\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        # Spelling error detection and correction\n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                # Take the first (best) suggestion\n",
    "                corrected_words.append(errors[word][0][0])\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "    \n",
    "    # def interactive_correction(self, text: str):\n",
    "    #     \"\"\"\n",
    "    #     Interactive spell checking with multiple suggestions\n",
    "    #     \"\"\"\n",
    "    #     errors = self.spell_check(text)\n",
    "        \n",
    "    #     if not errors:\n",
    "    #         return text\n",
    "        \n",
    "    #     print(\"Spelling Errors Detected:\")\n",
    "    #     for error, suggestions in errors.items():\n",
    "    #         print(f\"\\nMisspelled Word: {error}\")\n",
    "    #         print(\"Suggestions:\")\n",
    "    #         for i, (suggestion, score) in enumerate(suggestions, 1):\n",
    "    #             print(f\"{i}. {suggestion} (Similarity: {score}%)\")\n",
    "            \n",
    "    #         # User input for correction\n",
    "    #         while True:\n",
    "    #             try:\n",
    "    #                 choice = input(\"Enter the number of your chosen correction (or 0 to skip): \")\n",
    "    #                 choice = int(choice)\n",
    "                    \n",
    "    #                 if choice == 0:\n",
    "    #                     break\n",
    "    #                 elif 1 <= choice <= len(suggestions):\n",
    "    #                     text = text.replace(error, suggestions[choice-1][0])\n",
    "    #                     break\n",
    "    #                 else:\n",
    "    #                     print(\"Invalid choice. Please try again.\")\n",
    "    #             except ValueError:\n",
    "    #                 print(\"Please enter a valid number.\")\n",
    "        \n",
    "    #     return text\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt'\n",
    "    )\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\",  # Example with prefix variation\n",
    "        \"ගෘහෂ්ථ\",  # Mixed text\n",
    "        \"වෙරදි\"   # Another example\n",
    "    ]\n",
    "    \n",
    "    # Demonstrate different spell checking methods\n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "        \n",
    "        # Interactive correction (commented out to prevent blocking)\n",
    "        # corrected_text = spell_checker.interactive_correction(text)\n",
    "        # print(\"Interactively Corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0131b0ce-6c30-4244-a425-47d0ab3cad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියනවා', 72.33076923076922), ('කියවනය', 71.43076923076923), ('කියැවිය', 71.10714285714286)]}\n",
      "Auto-corrected Text: මම පොත කියනවා\n",
      "\n",
      "--- Spell Check for: අදරය\n",
      "Spelling Errors: {'අදරය': [('ආදරය', 92.5), ('දරය', 78.05357142857143), ('අරය', 78.05357142857143), ('ආදර්ශය', 75.5), ('ආදරණීය', 75.5)]}\n",
      "Auto-corrected Text: ආදරය\n",
      "\n",
      "--- Spell Check for: කැදරය\n",
      "Spelling Errors: {'කැදරය': [('කෑදරයා', 71.05681818181819)]}\n",
      "Auto-corrected Text: කෑදරයා\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import difflib\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str, stem_dictionary_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes\n",
    "        self.suffix_rules = self._load_suffixes(suffixes_path)\n",
    "        \n",
    "        # Load stemmer dictionary\n",
    "        self.stem_dictionary = self._load_stem_dictionary(stem_dictionary_path)\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _load_suffixes(self, suffixes_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Load suffixes from the file\n",
    "        \"\"\"\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            suffixes = file.read().splitlines()\n",
    "        return suffixes\n",
    "    \n",
    "    def _load_stem_dictionary(self, stem_dictionary_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Load stemmer dictionary from the file\n",
    "        \"\"\"\n",
    "        stem_dict = {}\n",
    "        with open(stem_dictionary_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                word, stem = line.strip().split('\\t')  # Assuming word and stem are tab-separated\n",
    "                stem_dict[word] = stem\n",
    "        return stem_dict\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def _apply_stemming(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Apply stemming using the stem dictionary\n",
    "        \"\"\"\n",
    "        return self.stem_dictionary.get(word, word)  # Return stemmed word if found, else original\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Apply stemming to the word before further processing\n",
    "        stemmed_word = self._apply_stemming(word)\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(stemmed_word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Apply stemming to dictionary words as well\n",
    "                stemmed_dict_word = self._apply_stemming(dict_word)\n",
    "                \n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        # Spelling error detection and correction\n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                # Take the first (best) suggestion\n",
    "                corrected_words.append(errors[word][0][0])\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt',\n",
    "        stem_dictionary_path='stem_dictionary.txt'  # Path to your stem dictionary file\n",
    "    )\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\",  # Example with prefix variation\n",
    "        \"අදරය\",  # Mixed text\n",
    "        \"කැදරය\"   # Another example\n",
    "    ]\n",
    "    \n",
    "    # Demonstrate different spell checking methods\n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "763f9a3a-0cbc-4ed0-ba0a-ee3c668ff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියවනවා', 84.77222222222221), ('කියවීම', 78.5), ('කියවෙනවා', 78.5), ('කියවනය', 76.85), ('කියූ', 72.9)]}\n",
      "Auto-corrected Text: මම පොත කියවනවා\n",
      "\n",
      "--- Spell Check for: ගෘහෂ්ථ\n",
      "Spelling Errors: {'ගෘහෂ්ථ': [('ගෘහස්ථ', 82.38333333333333)]}\n",
      "Auto-corrected Text: ගෘහස්ථ\n",
      "\n",
      "--- Spell Check for: අදරය\n",
      "Spelling Errors: {'අදරය': [('ආදරය', 92.5), ('අර', 74.0), ('දර', 74.0), ('අද', 74.0), ('ආර', 74.0)]}\n",
      "Auto-corrected Text: ආදරය\n"
     ]
    }
   ],
   "source": [
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes from suffixes_list.txt\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            self.suffix_rules = file.read().splitlines()\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies\n",
    "        \"\"\"\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker with suffix list file path\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt'\n",
    "    )\n",
    "    \n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\",\n",
    "        \"ගෘහෂ්ථ\",\n",
    "        \"අදරය\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dadc88f-3f68-4f92-9df6-8d2de6050975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: කඩ\n",
      "Spelling Errors: {}\n",
      "Auto-corrected Text: කඩ\n",
      "\n",
      "--- Spell Check for: ගෘහෂ්ථ\n",
      "Spelling Errors: {'ගෘහෂ්ථ': [('ගෘහස්ථ', 82.38333333333333)]}\n",
      "Auto-corrected Text: ගෘහස්ථ\n",
      "\n",
      "--- Spell Check for: යන්නෙයි\n",
      "Spelling Errors: {'යන්නෙයි': [('සොයන්නෙකි', 72.65833333333333), ('සොයන්නෙකුට', 72.65833333333333), ('සොයන්නෙක්', 72.65833333333333), ('සොයන්නෙකු', 72.65833333333333), ('සොයන්නෙම්', 72.65833333333333)]}\n",
      "Auto-corrected Text: සොයන්නෙකි\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import difflib\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str, stem_dictionary_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes from suffixes_list.txt\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            self.suffix_rules = file.read().splitlines()\n",
    "        \n",
    "        # Load stem dictionary\n",
    "        self.stem_dictionary = self._load_stem_dictionary(stem_dictionary_path)\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _load_stem_dictionary(self, stem_dictionary_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Load stem dictionary from a file, where each line contains a word variation and its stem.\n",
    "        \"\"\"\n",
    "        stem_dict = {}\n",
    "        with open(stem_dictionary_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                word, stem = line.strip().split('\\t')\n",
    "                stem_dict[word] = stem\n",
    "        return stem_dict\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies and stem dictionary\n",
    "        \"\"\"\n",
    "        # First, check if the word exists in the stem dictionary\n",
    "        if word in self.stem_dictionary:\n",
    "            return self.stem_dictionary[word]\n",
    "        \n",
    "        # If not, apply suffix removal rules\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker with suffix list and stem dictionary file paths\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt',\n",
    "        stem_dictionary_path='stem_dictionary.txt'\n",
    "    )\n",
    "    \n",
    "    test_texts = [\n",
    "        \"කඩ\",\n",
    "        \"ගෘහෂ්ථ\",\n",
    "        \"යන්නෙයි\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d52a1d32-4171-4b96-8c6c-a724eff5d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar error: Verb 'කියවම' should end with 'මු' when the subject is 'අපි'.\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "\n",
    "class SinhalaGrammarChecker:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = SinhalaTokenizer()\n",
    "        self.tagger = POSTagger()\n",
    "\n",
    "    def is_sov_order(self, pos_tags):\n",
    "        \"\"\"\n",
    "        Check if the sentence follows SOV order based on POS tags.\n",
    "        \"\"\"\n",
    "        if len(pos_tags) < 3:\n",
    "            return False  # Sentence too short to be SOV\n",
    "        \n",
    "        subject_tag, object_tag, verb_tag = pos_tags[0][1], pos_tags[1][1], pos_tags[2][1]\n",
    "        \n",
    "        # SOV structure: S -> PRP, O -> NNC, V -> V* (verbs starting with 'V')\n",
    "        return subject_tag == 'PRP' and object_tag == 'NNC' and verb_tag.startswith('V')\n",
    "\n",
    "    def check_grammar(self, sentence):\n",
    "        \"\"\"\n",
    "        Check grammar rules for a given sentence.\n",
    "        \"\"\"\n",
    "        tokenized_sentences = [self.tokenizer.tokenize(f'{ss}.') for ss in self.tokenizer.split_sentences(sentence)]\n",
    "        pos_tags = self.tagger.predict(tokenized_sentences)\n",
    "        \n",
    "        if not pos_tags or not pos_tags[0]:\n",
    "            return \"Unable to analyze the sentence.\"\n",
    "        \n",
    "        tokens = tokenized_sentences[0]\n",
    "        tags = pos_tags[0]\n",
    "        \n",
    "        # Ensure the sentence follows SOV structure\n",
    "        if not self.is_sov_order(tags):\n",
    "            return \"Sentence does not follow SOV order.\"\n",
    "        \n",
    "        # Extract Subject, Verb, and Object\n",
    "        subject = tokens[0]\n",
    "        verb = tokens[-2]\n",
    "        \n",
    "        # Rule 1: If S = \"මම\", V should end with \"මි\"\n",
    "        if subject == \"මම\" and not verb.endswith(\"මි\"):\n",
    "            return f\"Grammar error: Verb '{verb}' should end with 'මි' when the subject is 'මම'.\"\n",
    "        \n",
    "        # Rule 2: If S = \"අපි\", V should end with \"මු\"\n",
    "        if subject == \"අපි\" and not verb.endswith(\"මු\"):\n",
    "            return f\"Grammar error: Verb '{verb}' should end with 'මු' when the subject is 'අපි'.\"\n",
    "        \n",
    "        return \"The sentence is grammatically correct.\"\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    grammar_checker = SinhalaGrammarChecker()\n",
    "\n",
    "    sentence = \"අපි පොත කියවම \"\n",
    "    result = grammar_checker.check_grammar(sentence)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02542436-ffac-4cf8-a60e-e66db7e499e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger, word_splitter\n",
    "\n",
    "class SinhalaGrammarChecker:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = SinhalaTokenizer()\n",
    "        self.tagger = POSTagger()\n",
    "        \n",
    "    def get_verb_base(self, verb):\n",
    "        split_result = word_splitter.split(verb)\n",
    "        if split_result and 'base' in split_result and 'affix' in split_result:\n",
    "            return split_result['base'], split_result['affix']\n",
    "        return verb, ''\n",
    "        \n",
    "    def get_correct_suffix(self, subject):\n",
    "        suffix_map = {\n",
    "            \"මම\": \"මි\",\n",
    "            \"අපි\": \"මු\",\n",
    "        }\n",
    "        return suffix_map.get(subject, \"\")\n",
    "        \n",
    "    def create_corrected_verb(self, verb_base, correct_suffix):\n",
    "        \n",
    "        return f\"{verb_base}{correct_suffix}\"\n",
    "        \n",
    "    def is_sov_order(self, pos_tags):\n",
    "        \n",
    "        if len(pos_tags) < 3:\n",
    "            return False  # Sentence too short to be SOV\n",
    "        \n",
    "        subject_tag, object_tag, verb_tag = pos_tags[0][1], pos_tags[1][1], pos_tags[2][1]\n",
    "        return subject_tag == 'PRP' and object_tag == 'NNC' and verb_tag.startswith('V')\n",
    "        \n",
    "    def check_verb_agreement(self, subject, verb):\n",
    "\n",
    "        verb_base, affix = self.get_verb_base(verb)\n",
    "        correct_suffix = self.get_correct_suffix(subject)\n",
    "        \n",
    "        if not correct_suffix:\n",
    "            return True, \"\", None  # No rule for this subject\n",
    "            \n",
    "        if affix != correct_suffix:\n",
    "            corrected_verb = self.create_corrected_verb(verb_base, correct_suffix)\n",
    "            error_msg = f\"Grammar error: Verb '{verb}' (base: {verb_base}, affix: {affix}) should end with '{correct_suffix}' when subject is '{subject}'\"\n",
    "            return False, error_msg, corrected_verb\n",
    "            \n",
    "        return True, \"\", None\n",
    "        \n",
    "    def check_grammar(self, sentence):\n",
    "   \n",
    "        tokenized_sentences = [self.tokenizer.tokenize(f'{ss}.') for ss in self.tokenizer.split_sentences(sentence)]\n",
    "        pos_tags = self.tagger.predict(tokenized_sentences)\n",
    "        \n",
    "        if not pos_tags or not pos_tags[0]:\n",
    "            return \"Unable to analyze the sentence.\"\n",
    "        \n",
    "        tokens = tokenized_sentences[0]\n",
    "        tags = pos_tags[0]\n",
    "        \n",
    "        # Ensure the sentence follows SOV structure\n",
    "        if not self.is_sov_order(tags):\n",
    "            return \"Sentence does not follow SOV order.\"\n",
    "        \n",
    "        # Extract Subject and Verb\n",
    "        subject = tokens[0]\n",
    "        verb = tokens[-2]  # Assuming verb is second-to-last token\n",
    "        \n",
    "        # Check subject-verb agreement\n",
    "        is_valid, error_message, correction = self.check_verb_agreement(subject, verb)\n",
    "        if not is_valid:\n",
    "            # Create corrected sentence by replacing the verb\n",
    "            tokens[-2] = correction\n",
    "            corrected_sentence = ' '.join(tokens[:-1])  \n",
    "            return f\"{error_message}\\nSuggested correction: {corrected_sentence}\"\n",
    "            \n",
    "        return \"The sentence is grammatically correct.\"\n",
    "\n",
    "def main():\n",
    "    grammar_checker = SinhalaGrammarChecker()\n",
    "    \n",
    "    print(\"Testing grammar checking with corrections:\")\n",
    "    test_sentences = [\n",
    "        \"මම ගෙදර යයි\",     # Incorrect: Wrong verb ending for මම\n",
    "        \"අපි කඩේට යයි \",     # Incorrect: Wrong verb ending for අපි\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        print(f\"\\nChecking sentence: {sentence}\")\n",
    "        result = grammar_checker.check_grammar(sentence)\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4553d2dd-89a5-4276-944d-fe47b2793680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sinling\n",
      "  Downloading sinling-0.3.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting emoji (from sinling)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from sinling) (1.4.2)\n",
      "Collecting pygtrie (from sinling)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting sklearn-crfsuite (from sinling)\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from sinling) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from nltk->sinling) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from nltk->sinling) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from nltk->sinling) (4.66.5)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->sinling)\n",
      "  Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from sklearn-crfsuite->sinling) (1.5.2)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in d:\\anaconda\\lib\\site-packages (from sklearn-crfsuite->sinling) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk->sinling) (0.4.6)\n",
      "Downloading sinling-0.3.6-py3-none-any.whl (20.0 MB)\n",
      "   ---------------------------------------- 0.0/20.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/20.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/20.0 MB 4.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.0/20.0 MB 3.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.6/20.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.4/20.0 MB 2.0 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 4.2/20.0 MB 2.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.2/20.0 MB 2.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.8/20.0 MB 2.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 6.3/20.0 MB 2.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 7.3/20.0 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 8.4/20.0 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 9.2/20.0 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 9.4/20.0 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 9.7/20.0 MB 2.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 11.3/20.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 12.6/20.0 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 13.9/20.0 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 16.3/20.0 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 17.3/20.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.8/20.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.9/20.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.4/20.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.0/20.0 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 586.9/586.9 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Installing collected packages: pygtrie, python-crfsuite, emoji, sklearn-crfsuite, sinling\n",
      "Successfully installed emoji-2.14.0 pygtrie-2.5.0 python-crfsuite-0.9.11 sinling-0.3.6 sklearn-crfsuite-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at d:\\anaconda\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sinling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8906f93e-b8b4-4663-a1de-7635d1404507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('මම', 'PRP'), ('පොත', 'NNC'), ('කියවයි.', 'NNC')]]\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "\n",
    "tokenizer = SinhalaTokenizer()\n",
    "tagger = POSTagger()\n",
    "\n",
    "document = \"මම පොත කියවයි\"\n",
    "tokenized_sentences = [tokenizer.tokenize(f'{ss}.') for ss in tokenizer.split_sentences(document)]\n",
    "pos_tags = tagger.predict(tokenized_sentences)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943d3e63-55f8-44dd-bc13-207a07e22cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinling import word_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1573d16b-93e0-4022-b31d-c96018a2526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'debug': [['ග', 'නිමු  ', 1.9932885906040267],\n",
       "  ['ගන', 'ිමු  ', 1.9951456310679612],\n",
       "  ['ගනි', 'මු  ', 1.9902439024390244],\n",
       "  ['ගනිම්', 'ු  ', 1.6666666666666667],\n",
       "  ['ගනිමු', '  ', 1.9988109393579072],\n",
       "  ['ගනිමු ', ' ', 1.0]],\n",
       " 'base': 'ගනිමු',\n",
       " 'affix': '  '}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_splitter.split('ගනිමු')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9531f0df-d392-4922-a5c3-a29942a6be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Input Sentences...\n",
      "\n",
      "Processing: මම ගදර යයි \n",
      "Text after Spell Checking: මම ගෙදර යයි\n",
      "Running Grammar Checker...\n",
      "\n",
      "Processing: අපි කඩේට යයි\n",
      "Text after Spell Checking: අපි කඩේ යයි\n",
      "Running Grammar Checker...\n",
      "\n",
      "Processing: අපි අදරය කරයි\n",
      "Text after Spell Checking: අපි ආදරය කරයි\n",
      "Running Grammar Checker...\n",
      "\n",
      "Processing: මම කෑම ගනිමු\n",
      "Text after Spell Checking: මම කෑම ගනිමු\n",
      "Running Grammar Checker...\n",
      "\n",
      "Processing: අපි රට යයි\n",
      "Text after Spell Checking: අපි රට යයි\n",
      "Running Grammar Checker...\n",
      "\n",
      "Processing: අපි කලි යම\n",
      "Text after Spell Checking: අපි කලින් යම\n",
      "Running Grammar Checker...\n",
      "\n",
      "Final Output:\n",
      "Sentence 1: Grammar error: Verb 'යයි' (base: ය, affix: යි) should end with 'මි' when subject is 'මම'\n",
      "Suggested correction: මම ගෙදර යමි\n",
      "Sentence 2: Grammar error: Verb 'යයි' (base: ය, affix: යි) should end with 'මු' when subject is 'අපි'\n",
      "Suggested correction: අපි කඩේ යමු\n",
      "Sentence 3: Grammar error: Verb 'කරයි' (base: කර, affix: යි) should end with 'මු' when subject is 'අපි'\n",
      "Suggested correction: අපි ආදරය කරමු\n",
      "Sentence 4: Grammar error: Verb 'ගනිමු' (base: ගනි, affix: මු) should end with 'මි' when subject is 'මම'\n",
      "Suggested correction: මම කෑම ගනිමි\n",
      "Sentence 5: Grammar error: Verb 'යයි' (base: ය, affix: යි) should end with 'මු' when subject is 'අපි'\n",
      "Suggested correction: අපි රට යමු\n",
      "Sentence 6: Grammar error: Verb 'යම' (base: ය, affix: ම) should end with 'මු' when subject is 'අපි'\n",
      "Suggested correction: අපි කලින් යමු\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sinhala_spell_checker import SinhalaSpellChecker\n",
    "from sinhala_grammar_checker import SinhalaGrammarChecker\n",
    "\n",
    "\n",
    "class SinhalaTextProcessor:\n",
    "    def __init__(self, spell_checker_config: dict):\n",
    "        \"\"\"\n",
    "        Initialize the text processor with spell checker and grammar checker.\n",
    "        \"\"\"\n",
    "        self.spell_checker = SinhalaSpellChecker(\n",
    "            dictionary_path=spell_checker_config[\"dictionary_path\"],\n",
    "            stopwords_path=spell_checker_config[\"stopwords_path\"],\n",
    "            suffixes_path=spell_checker_config[\"suffixes_path\"],\n",
    "            stem_dictionary_path=spell_checker_config[\"stem_dictionary_path\"]\n",
    "        )\n",
    "        self.grammar_checker = SinhalaGrammarChecker()\n",
    "\n",
    "    def process_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Process the input text by first correcting the spelling of the middle word,\n",
    "        then checking grammar.\n",
    "        \"\"\"\n",
    "        # Step 1: Extract and correct the middle word (assumed to be the object)\n",
    "        words = text.split()\n",
    "        if len(words) < 3:\n",
    "            return \"Sentence too short. Please provide a sentence with Subject, Object, and Verb.\"\n",
    "        \n",
    "        middle_word_index = len(words) // 2\n",
    "        middle_word = words[middle_word_index]\n",
    "\n",
    "        corrected_word = self.spell_checker.auto_correct(middle_word)\n",
    "\n",
    "        # Replace the middle word in the original sentence\n",
    "        words[middle_word_index] = corrected_word\n",
    "        corrected_text = ' '.join(words)\n",
    "        print(f\"Text after Spell Checking: {corrected_text}\")\n",
    "\n",
    "        # Step 2: Grammar Check\n",
    "        print(\"Running Grammar Checker...\")\n",
    "        grammar_result = self.grammar_checker.check_grammar(corrected_text)\n",
    "\n",
    "        return grammar_result\n",
    "\n",
    "\n",
    "# Configuration for the spell checker\n",
    "spell_checker_config = {\n",
    "    \"dictionary_path\": 'data-spell-checker.xlsx',  # Path to the dictionary file\n",
    "    \"stopwords_path\": 'stop words.txt',            # Path to the stopwords file\n",
    "    \"suffixes_path\": \"suffixes_list.txt\",         # Path to the suffixes list file\n",
    "    \"stem_dictionary_path\": \"stem_dictionary.txt\" # Path to the stem dictionary file\n",
    "}\n",
    "\n",
    "# Ensure all files exist\n",
    "for file_path in spell_checker_config.values():\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "# Initialize the Sinhala Text Processor\n",
    "text_processor = SinhalaTextProcessor(spell_checker_config)\n",
    "\n",
    "# Input Sentences\n",
    "input_sentences = [\n",
    "    \"මම ගදර යයි \",       \n",
    "    \"අපි කඩේට යයි\",\n",
    "    \"අපි අදරය කරයි\",\n",
    "    \"මම කෑම ගනිමු\",\n",
    "    \"අපි රට යයි\",\n",
    "    \"අපි කලි යම\"\n",
    "]\n",
    "\n",
    "# Process each sentence\n",
    "print(\"\\nProcessing Input Sentences...\")\n",
    "results = []\n",
    "for sentence in input_sentences:\n",
    "    print(f\"\\nProcessing: {sentence}\")\n",
    "    result = text_processor.process_text(sentence)\n",
    "    results.append(result)\n",
    "\n",
    "# Final Output\n",
    "print(\"\\nFinal Output:\")\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"Sentence {i + 1}: {res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4bc4f-7bf4-4c5f-887c-e8a010827ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
