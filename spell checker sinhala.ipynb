{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35b30d8-fe70-4305-be39-f8b9bf43cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 67260 correct Sinhala words.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel dataset\n",
    "file_path = \"data-spell-checker.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Extract correct words\n",
    "dictionary = data[data['label'] == 1]['word'].tolist()\n",
    "print(f\"Loaded {len(dictionary)} correct Sinhala words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6e0fac-42d2-493d-a590-21ef0155bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sinhala stop words\n",
    "with open('stop words.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4affbe2b-693f-4bff-99ce-18bd23af4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    # Use regex to remove non-alphanumeric characters except spaces\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation (keeping spaces)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = cleaned_text.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec106113-e843-4ff6-a223-39284d6dd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinhala_stemmer(word):\n",
    "    suffixes = ['ින්', 'ට', 'ව', 'ගේ', 'යන්', 'නවා']  # Add more relevant suffixes\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fbd918d-7188-49f8-9cbc-3c0283a4af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spelling_errors(words, dictionary):\n",
    "    # Find words not in the dictionary\n",
    "    errors = [word for word in words if word not in dictionary]\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd125c02-89c5-4c4a-a712-38f17e8927e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def sinhala_soundex(word):\n",
    "    phonetic_map = {\n",
    "        'ක': '1', 'ඛ': '1',\n",
    "        'ච': '2', 'ජ': '2', 'ඡ': '2', 'ඣ': '2',\n",
    "        'ට': '3', 'ඩ': '3', 'ඨ': '3', 'ඪ': '3',\n",
    "        'ත': '4', 'ථ': '4',\n",
    "        'බ': '5', 'ඵ': '5', 'භ': '5',\n",
    "        'ශ': '7', 'ෂ': '7', 'ස': '7',\n",
    "        'ග': '8', 'ඝ': '8', 'ඟ': '8'\n",
    "    }\n",
    "    first_letter = word[0]\n",
    "    soundex_code = [first_letter]\n",
    "    for char in word[1:]:\n",
    "        if char in phonetic_map:\n",
    "            code = phonetic_map[char]\n",
    "            if soundex_code[-1] != code:\n",
    "                soundex_code.append(code)\n",
    "    while len(soundex_code) < 4:\n",
    "        soundex_code.append('0')  # Pad with zeros\n",
    "    return ''.join(soundex_code[:4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7692da54-010d-491a-a0d1-da1ea6938c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections(errors, dictionary, limit=3, threshold=80):\n",
    "    suggestions = {}\n",
    "    for error in errors:\n",
    "        matches = process.extract(error, dictionary, limit=limit)\n",
    "        # Filter suggestions based on minimum similarity threshold\n",
    "        filtered_matches = [match[0] for match in matches if match[1] >= threshold]\n",
    "        suggestions[error] = filtered_matches\n",
    "    return suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c33d4c-a1f5-45ab-b35a-adf6fd325a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(input_text, dictionary, stopwords):\n",
    "    # Step 1: Preprocess input text\n",
    "    words = preprocess_text(input_text, stopwords)\n",
    "    # Step 2: Detect spelling errors\n",
    "    errors = detect_spelling_errors(words, dictionary)\n",
    "    \n",
    "    if not errors:\n",
    "        return \"No spelling errors found!\", {}\n",
    "\n",
    "    # Step 3: Suggest corrections for detected errors\n",
    "    corrections = suggest_corrections(errors, dictionary)\n",
    "    return errors, corrections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "328936b9-98af-461d-83a7-9ceb36d1cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(input_text, dictionary, stopwords):\n",
    "    errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
    "    words = input_text.split()\n",
    "\n",
    "    # Replace each word with the top suggestion if available\n",
    "    corrected_words = [\n",
    "        corrections.get(word, [word])[0]  # If word is found in corrections, replace it\n",
    "        for word in words\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(corrected_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "408667bd-6f31-44e2-b7fb-ce156f12c52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: ['කක']\n",
      "Suggestions: {'කක': ['අධිකකම', 'අවංකකම', 'එකක්වත්']}\n",
      "Corrected Text: කේක\n"
     ]
    }
   ],
   "source": [
    "input_text = \"කේක\"\n",
    "\n",
    "# Run the spell checker\n",
    "errors, corrections = spell_checker(input_text, dictionary, stopwords)\n",
    "print(\"Errors:\", errors)\n",
    "print(\"Suggestions:\", corrections)\n",
    "\n",
    "# Auto-correct the text\n",
    "corrected_text = auto_correct(input_text, dictionary, stopwords)\n",
    "print(\"Corrected Text:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9046ac9e-0808-4291-bd24-77dc19a8d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Errors: {'අදරය': [('දරය', 86.0), ('අරය', 86.0), ('අධරය', 85.0), ('අනාදරය', 80.0), ('අන්දරය', 80.0)]}\n",
      "Corrected Text: දරය\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class AdvancedSinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str):\n",
    "        # Load dictionary with more sophisticated preprocessing\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Suffix rules for more comprehensive stemming\n",
    "        self.suffix_rules = [\n",
    "            'ආගම', 'ගෙන', 'යෙහි', 'යේ', 'ට', 'ම', \n",
    "            'යන', 'ක', 'වා', 'ලා', 'ල', 'න', 'හි'\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "      \n",
    "        # Assuming 'word' column contains correct words and 'label' column indicates correctness\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "       \n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "      \n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "     \n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "    \n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Stemming\n",
    "        stemmed_word = self._advanced_stemmer(word)\n",
    "        \n",
    "        # Multiple similarity strategies\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            # Phonetic similarity\n",
    "            phonetic_similarity = fuzz.ratio(\n",
    "                self._phonetic_key(stemmed_word), \n",
    "                self._phonetic_key(dict_word)\n",
    "            )\n",
    "            \n",
    "            # String-based similarity\n",
    "            string_similarity = fuzz.ratio(word, dict_word)\n",
    "            \n",
    "            # Levenshtein distance\n",
    "            edit_similarity = fuzz.token_sort_ratio(word, dict_word)\n",
    "            \n",
    "            # Combined weighted similarity\n",
    "            combined_score = (\n",
    "                0.4 * phonetic_similarity + \n",
    "                0.3 * string_similarity + \n",
    "                0.3 * edit_similarity\n",
    "            )\n",
    "            \n",
    "            candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort and filter candidates\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [\n",
    "            (candidate, score) \n",
    "            for candidate, score in candidates \n",
    "            if score >= threshold\n",
    "        ][:limit]\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "       \n",
    "        # Preprocess text\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        # Spelling error detection and correction\n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "      \n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                # Take the first (best) suggestion\n",
    "                corrected_words.append(errors[word][0][0])\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker\n",
    "    spell_checker = AdvancedSinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt'\n",
    "    )\n",
    "    \n",
    "    # Test input\n",
    "    test_text = \"අදරය\"\n",
    "    \n",
    "    # Spell check\n",
    "    spelling_errors = spell_checker.spell_check(test_text)\n",
    "    print(\"Spelling Errors:\", spelling_errors)\n",
    "    \n",
    "    # Auto-correction\n",
    "    corrected_text = spell_checker.auto_correct(test_text)\n",
    "    print(\"Corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4553d2dd-89a5-4276-944d-fe47b2793680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sinling\n",
      "  Downloading sinling-0.3.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting emoji (from sinling)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from sinling) (1.4.2)\n",
      "Collecting pygtrie (from sinling)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting sklearn-crfsuite (from sinling)\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from sinling) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from nltk->sinling) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from nltk->sinling) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from nltk->sinling) (4.66.5)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->sinling)\n",
      "  Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from sklearn-crfsuite->sinling) (1.5.2)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in d:\\anaconda\\lib\\site-packages (from sklearn-crfsuite->sinling) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite->sinling) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\salin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk->sinling) (0.4.6)\n",
      "Downloading sinling-0.3.6-py3-none-any.whl (20.0 MB)\n",
      "   ---------------------------------------- 0.0/20.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/20.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/20.0 MB 4.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.0/20.0 MB 3.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.6/20.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.1/20.0 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.4/20.0 MB 2.0 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 4.2/20.0 MB 2.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.2/20.0 MB 2.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 5.8/20.0 MB 2.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 6.3/20.0 MB 2.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 7.3/20.0 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 8.4/20.0 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 9.2/20.0 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 9.4/20.0 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 9.7/20.0 MB 2.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 11.3/20.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 12.6/20.0 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 13.9/20.0 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/20.0 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 15.7/20.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 16.3/20.0 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 17.3/20.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.8/20.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.9/20.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.4/20.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.0/20.0 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 586.9/586.9 kB 6.9 MB/s eta 0:00:00\n",
      "Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Installing collected packages: pygtrie, python-crfsuite, emoji, sklearn-crfsuite, sinling\n",
      "Successfully installed emoji-2.14.0 pygtrie-2.5.0 python-crfsuite-0.9.11 sinling-0.3.6 sklearn-crfsuite-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at d:\\anaconda\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sinling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8906f93e-b8b4-4663-a1de-7635d1404507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('මම', 'PRP'), ('පොත', 'NNC'), ('කියවායි', 'VFM'), ('.', 'FS')]]\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "\n",
    "tokenizer = SinhalaTokenizer()\n",
    "tagger = POSTagger()\n",
    "\n",
    "document = \"මම පොත කියවායි\"\n",
    "tokenized_sentences = [tokenizer.tokenize(f'{ss}.') for ss in tokenizer.split_sentences(document)]\n",
    "pos_tags = tagger.predict(tokenized_sentences)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "763f9a3a-0cbc-4ed0-ba0a-ee3c668ff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියවනවා', 84.77222222222221), ('කියවීම', 78.5), ('කියවෙනවා', 78.5), ('කියවනය', 76.85), ('කියූ', 72.9)]}\n",
      "Auto-corrected Text: මම පොත කියවනවා\n",
      "\n",
      "--- Spell Check for: ගෘහෂ්ථ\n",
      "Spelling Errors: {'ගෘහෂ්ථ': [('ගෘහස්ථ', 82.38333333333333)]}\n",
      "Auto-corrected Text: ගෘහස්ථ\n",
      "\n",
      "--- Spell Check for: අදරය\n",
      "Spelling Errors: {'අදරය': [('ආදරය', 92.5), ('අර', 74.0), ('දර', 74.0), ('අද', 74.0), ('ආර', 74.0)]}\n",
      "Auto-corrected Text: ආදරය\n"
     ]
    }
   ],
   "source": [
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes from suffixes_list.txt\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            self.suffix_rules = file.read().splitlines()\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies\n",
    "        \"\"\"\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker with suffix list file path\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt'\n",
    "    )\n",
    "    \n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\",\n",
    "        \"ගෘහෂ්ථ\",\n",
    "        \"අදරය\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dadc88f-3f68-4f92-9df6-8d2de6050975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spell Check for: මම පොත කියවායි\n",
      "Spelling Errors: {'කියවායි': [('කියවනවා', 84.77222222222221), ('කියවීම', 78.5), ('කියවනය', 76.85), ('කියූ', 72.9), ('කියනවා', 72.9)]}\n",
      "Auto-corrected Text: මම පොත කියවනවා\n"
     ]
    }
   ],
   "source": [
    "class SinhalaSpellChecker:\n",
    "    def __init__(self, dictionary_path: str, stopwords_path: str, suffixes_path: str, stem_dictionary_path: str):\n",
    "        # Load dictionary\n",
    "        self.data = pd.read_excel(dictionary_path)\n",
    "        self.dictionary = self._preprocess_dictionary()\n",
    "        \n",
    "        # Load stopwords\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "            self.stopwords = set(file.read().splitlines())\n",
    "        \n",
    "        # Load suffixes from suffixes_list.txt\n",
    "        with open(suffixes_path, 'r', encoding='utf-8') as file:\n",
    "            self.suffix_rules = file.read().splitlines()\n",
    "        \n",
    "        # Load stem dictionary\n",
    "        self.stem_dictionary = self._load_stem_dictionary(stem_dictionary_path)\n",
    "        \n",
    "        # Advanced phonetic mapping\n",
    "        self.phonetic_mapping = self._create_advanced_phonetic_mapping()\n",
    "        \n",
    "        # Prefix and suffix variations\n",
    "        self.prefix_variations = {\n",
    "            'අ': ['ආ', 'අ'],\n",
    "            'අද': ['ආද', 'අද'],\n",
    "            'අන': ['ආන', 'අන']\n",
    "        }\n",
    "    \n",
    "    def _load_stem_dictionary(self, stem_dictionary_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Load stem dictionary from a file, where each line contains a word variation and its stem.\n",
    "        \"\"\"\n",
    "        stem_dict = {}\n",
    "        with open(stem_dictionary_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                word, stem = line.strip().split('\\t')\n",
    "                stem_dict[word] = stem\n",
    "        return stem_dict\n",
    "    \n",
    "    def _preprocess_dictionary(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced dictionary preprocessing\n",
    "        \"\"\"\n",
    "        correct_words = self.data[self.data['label'] == 1]['word']\n",
    "        \n",
    "        # Remove duplicates, convert to lowercase, remove special characters\n",
    "        processed_words = set(\n",
    "            re.sub(r'[^\\u0D80-\\u0DFF]', '', word.lower()) \n",
    "            for word in correct_words\n",
    "        )\n",
    "        \n",
    "        return list(processed_words)\n",
    "    \n",
    "    def _create_advanced_phonetic_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Comprehensive phonetic mapping for Sinhala characters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Consonant groups with similar sounds\n",
    "            'ක': 'k', 'ඛ': 'k', 'ගෑ': 'g', 'ඝ': 'g',\n",
    "            'ච': 'c', 'ජ': 'j', 'ඣ': 'j',\n",
    "            'ට': 't', 'ඩ': 'd', 'ඨ': 't', 'ඪ': 'd',\n",
    "            'ත': 't', 'ද': 'd', 'ධ': 'd',\n",
    "            'ප': 'p', 'බ': 'b', 'භ': 'b',\n",
    "            'ම': 'm', 'න': 'n', 'ණ': 'n',\n",
    "            'ල': 'l', 'ළ': 'l',\n",
    "            'ර': 'r', 'ඍ': 'r',\n",
    "            'ව': 'v', 'ශ': 's', 'ෂ': 's', 'ස': 's', \n",
    "            'හ': 'h'\n",
    "        }\n",
    "    \n",
    "    def _advanced_stemmer(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Advanced stemming with multiple suffix removal strategies and stem dictionary\n",
    "        \"\"\"\n",
    "        # First, check if the word exists in the stem dictionary\n",
    "        if word in self.stem_dictionary:\n",
    "            return self.stem_dictionary[word]\n",
    "        \n",
    "        # If not, apply suffix removal rules\n",
    "        original_word = word\n",
    "        for suffix in self.suffix_rules:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        \n",
    "        # If no suffix removed and word is too short, return original\n",
    "        return word if len(word) > 2 else original_word\n",
    "    \n",
    "    def _phonetic_key(self, word: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate advanced phonetic key\n",
    "        \"\"\"\n",
    "        phonetic_key = ''\n",
    "        for char in word:\n",
    "            phonetic_key += self.phonetic_mapping.get(char, char)\n",
    "        return phonetic_key\n",
    "    \n",
    "    def _generate_prefix_variations(self, word: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate potential prefix variations of a word\n",
    "        \"\"\"\n",
    "        variations = [word]\n",
    "        \n",
    "        for prefix, alternates in self.prefix_variations.items():\n",
    "            if word.startswith(prefix):\n",
    "                for alt_prefix in alternates:\n",
    "                    if prefix != alt_prefix:\n",
    "                        variation = alt_prefix + word[len(prefix):]\n",
    "                        variations.append(variation)\n",
    "        \n",
    "        return variations\n",
    "    \n",
    "    def find_corrections(self, word: str, limit: int = 5, threshold: int = 70) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Enhanced correction finding with prefix variations and multiple similarity metrics\n",
    "        \"\"\"\n",
    "        # Check if word is already correct\n",
    "        if word in self.stopwords or word in self.dictionary:\n",
    "            return [(word, 100)]\n",
    "        \n",
    "        # Generate prefix variations to check\n",
    "        word_variations = self._generate_prefix_variations(word)\n",
    "        \n",
    "        # Comprehensive similarity calculation\n",
    "        candidates = []\n",
    "        for dict_word in self.dictionary:\n",
    "            for variation in word_variations:\n",
    "                # Stem both variation and dictionary word\n",
    "                stemmed_variation = self._advanced_stemmer(variation)\n",
    "                stemmed_dict_word = self._advanced_stemmer(dict_word)\n",
    "                \n",
    "                # Multiple similarity metrics\n",
    "                phonetic_similarity = fuzz.ratio(\n",
    "                    self._phonetic_key(stemmed_variation), \n",
    "                    self._phonetic_key(stemmed_dict_word)\n",
    "                )\n",
    "                \n",
    "                string_similarity = fuzz.ratio(stemmed_variation, stemmed_dict_word)\n",
    "                edit_similarity = fuzz.token_sort_ratio(stemmed_variation, stemmed_dict_word)\n",
    "                \n",
    "                # Sequence matcher for more nuanced similarity\n",
    "                seq_matcher = difflib.SequenceMatcher(None, stemmed_variation, stemmed_dict_word)\n",
    "                sequence_similarity = seq_matcher.ratio() * 100\n",
    "                \n",
    "                # Prefix similarity\n",
    "                prefix_similarity = fuzz.ratio(variation[:3], dict_word[:3]) * 0.5\n",
    "                \n",
    "                # Combined weighted similarity\n",
    "                combined_score = (\n",
    "                    0.25 * phonetic_similarity + \n",
    "                    0.2 * string_similarity + \n",
    "                    0.15 * edit_similarity +\n",
    "                    0.25 * sequence_similarity +\n",
    "                    0.15 * prefix_similarity\n",
    "                )\n",
    "                \n",
    "                candidates.append((dict_word, combined_score))\n",
    "        \n",
    "        # Sort, filter, and limit candidates\n",
    "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        unique_candidates = []\n",
    "        seen = set()\n",
    "        for candidate, score in candidates:\n",
    "            if candidate not in seen and score >= threshold:\n",
    "                unique_candidates.append((candidate, score))\n",
    "                seen.add(candidate)\n",
    "                if len(unique_candidates) == limit:\n",
    "                    break\n",
    "        \n",
    "        return unique_candidates\n",
    "    \n",
    "    def spell_check(self, text: str) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Comprehensive spell checking\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+', text)\n",
    "        \n",
    "        spelling_errors = {}\n",
    "        for word in words:\n",
    "            if word not in self.dictionary and word not in self.stopwords:\n",
    "                corrections = self.find_corrections(word)\n",
    "                if corrections:\n",
    "                    spelling_errors[word] = corrections\n",
    "        \n",
    "        return spelling_errors\n",
    "    \n",
    "    def auto_correct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Automatically correct text using best suggestions\n",
    "        \"\"\"\n",
    "        errors = self.spell_check(text)\n",
    "        corrected_words = []\n",
    "        \n",
    "        for word in text.split():\n",
    "            if word in errors:\n",
    "                corrected_words.append(errors[word][0][0])  # Take the first suggestion\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "        \n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    # Initialize spell checker with suffix list and stem dictionary file paths\n",
    "    spell_checker = SinhalaSpellChecker(\n",
    "        dictionary_path='data-spell-checker.xlsx',\n",
    "        stopwords_path='stop words.txt',\n",
    "        suffixes_path='suffixes_list.txt',\n",
    "        stem_dictionary_path='stem_dictionary.txt'\n",
    "    )\n",
    "    \n",
    "    test_texts = [\n",
    "        \"මම පොත කියවායි\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\n--- Spell Check for:\", text)\n",
    "        \n",
    "        # Find spelling errors\n",
    "        errors = spell_checker.spell_check(text)\n",
    "        print(\"Spelling Errors:\", errors)\n",
    "        \n",
    "        # Auto-correction\n",
    "        corrected_text = spell_checker.auto_correct(text)\n",
    "        print(\"Auto-corrected Text:\", corrected_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d52a1d32-4171-4b96-8c6c-a724eff5d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar error: Verb '.' should end with 'මි' when the subject is 'මම'.\n"
     ]
    }
   ],
   "source": [
    "from sinling import SinhalaTokenizer, POSTagger\n",
    "\n",
    "class SinhalaGrammarChecker:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = SinhalaTokenizer()\n",
    "        self.tagger = POSTagger()\n",
    "\n",
    "    def is_sov_order(self, pos_tags):\n",
    "        \"\"\"\n",
    "        Check if the sentence follows SOV order based on POS tags.\n",
    "        \"\"\"\n",
    "        if len(pos_tags) < 3:\n",
    "            return False  # Sentence too short to be SOV\n",
    "        \n",
    "        subject_tag, object_tag, verb_tag = pos_tags[0][1], pos_tags[1][1], pos_tags[2][1]\n",
    "        \n",
    "        # SOV structure: S -> PRP, O -> NNC, V -> V* (verbs starting with 'V')\n",
    "        return subject_tag == 'PRP' and object_tag == 'NNC' and verb_tag.startswith('V')\n",
    "\n",
    "    def check_grammar(self, sentence):\n",
    "        \"\"\"\n",
    "        Check grammar rules for a given sentence.\n",
    "        \"\"\"\n",
    "        tokenized_sentences = [self.tokenizer.tokenize(f'{ss}.') for ss in self.tokenizer.split_sentences(sentence)]\n",
    "        pos_tags = self.tagger.predict(tokenized_sentences)\n",
    "        \n",
    "        if not pos_tags or not pos_tags[0]:\n",
    "            return \"Unable to analyze the sentence.\"\n",
    "        \n",
    "        tokens = tokenized_sentences[0]\n",
    "        tags = pos_tags[0]\n",
    "        \n",
    "        # Ensure the sentence follows SOV structure\n",
    "        if not self.is_sov_order(tags):\n",
    "            return \"Sentence does not follow SOV order.\"\n",
    "        \n",
    "        # Extract Subject, Verb, and Object\n",
    "        subject = tokens[0]\n",
    "        verb = tokens[-1]\n",
    "        \n",
    "        # Rule 1: If S = \"මම\", V should end with \"මි\"\n",
    "        if subject == \"මම\" and not verb.endswith(\"මි\"):\n",
    "            return f\"Grammar error: Verb '{verb}' should end with 'මි' when the subject is 'මම'.\"\n",
    "        \n",
    "        # Rule 2: If S = \"අපි\", V should end with \"මු\"\n",
    "        if subject == \"අපි\" and not verb.endswith(\"මු\"):\n",
    "            return f\"Grammar error: Verb '{verb}' should end with 'මු' when the subject is 'අපි'.\"\n",
    "        \n",
    "        return \"The sentence is grammatically correct.\"\n",
    "\n",
    "# Example Usage\n",
    "def main():\n",
    "    grammar_checker = SinhalaGrammarChecker()\n",
    "\n",
    "    sentence = \"මම පොත කියවායි\"\n",
    "    result = grammar_checker.check_grammar(sentence)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610e661-8cce-479b-a931-fdeafa7dbe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
